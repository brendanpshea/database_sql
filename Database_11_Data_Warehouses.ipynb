{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxbs4YWpRlYkpLEJWLkgQ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brendanpshea/database_sql/blob/main/Database_11_Data_Warehouses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Data Warehouses\n",
        "### Brendan Shea, PhD\n",
        "\n",
        "A **Data Warehouse** is a specific kind of database designed to help people like analysts, data scientists, and decision-makers get answers to complex questions. Unlike a traditional \"application\" database, which is geared towards handling daily transactions like adding new customer records or updating product prices, a data warehouse is optimized for digging deep into data to reveal patterns or trends. For instance, a data warehouse could help a business figure out whether sales are better in summer or winter, across multiple years and regions.\n",
        "\n",
        "The key difference between a data warehouse and a traditional application database lies in their primary functions. A traditional database is like the cash register in a store; it's essential for day-to-day operations, keeping track of each transaction as it happens. It's designed to handle lots of quick, simple queries, like checking the price of an item or updating the quantity of a product in stock. Because it's transaction-oriented, it's structured to make these operations as fast and efficient as possible.\n",
        "\n",
        "In contrast, a data warehouse is more like the store's back office, where managers and analysts look over sales data, inventory levels, and customer feedback to make longer-term decisions. It's structured to handle fewer queries, but these queries are much more complex and pull data from multiple sources. This is why data warehouses often use a different kind of data organization, like denormalization and Star Schema, to make these complex queries faster to run."
      ],
      "metadata": {
        "id": "kge4Ujm7l2Bp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to the Zombies! Case Study\n",
        "\n",
        "In this case study, you find yourself in the role of a data scientist at the Center for Disease Control and Prevention (CDC), the national public health agency of the United States. Your mission is urgent and of the highest importance: a Zombie Outbreak has occurred, and it's spreading at an alarming rate. Traditional methods of containment have proven ineffective, and the situation is escalating. The CDC has been tasked with synthesizing data from a myriad of sources to develop a strategy for containment and, ultimately, eradication of this unprecedented threat.\n",
        "\n",
        "The data you'll be working with comes from diverse origins---hospital records, military intelligence, social media feeds, satellite imagery, and more. Each data source provides a piece of the puzzle, and it's your job to assemble these pieces into a coherent and actionable picture. You'll be using Postgres as your data warehousing solution to integrate these disparate data sets.\n",
        "\n",
        "In this context, application databases might help hospitals keep track of current patient records and immediate resource allocation. But a data warehouse would help the CDC analyze trends across *many* communities, like how the outbreak is spreading across regions, the effectiveness of various treatments over time, or how different variables like weather or population density affect the rate of infection\n",
        "\n",
        "The stakes are high. Decisions based on your analyses will directly impact the deployment of resources, from medical supplies to military interventions. Time is of the essence, and the tools at your disposal, including the data warehouse, must be wielded with both speed and precision. This case study will guide you through the key questions and challenges you'll face in this high-pressure scenario, providing you with the technical know-how to navigate the complexities of data warehousing with Postgres in the context of a global emergency."
      ],
      "metadata": {
        "id": "pgtDPm6em_0a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Sources\n",
        "\n",
        "Data Warehouses are typically constructed by pulling in data from many other sources, including (but not limited to) other databases. During a Zombie Outbreak, the CDC would likely draw from a diverse array of data sources to form a comprehensive understanding of the situation. These sources would include:\n",
        "\n",
        "1.  *Hospital Records*  would provide real-time information on the number of infected individuals, their symptoms, and the effectiveness of treatments. This is analogous to Electronic Health Records (EHR) commonly integrated into healthcare data warehouses.\n",
        "\n",
        "2.  *Military Intelligence* reports from units engaged with zombie containment would offer insights into the effectiveness of various strategies and the spread of the outbreak. In real-world scenarios, defense departments often maintain their own data warehouses for strategic analysis.\n",
        "\n",
        "3.  To track large-scale movements of zombie hordes, *satellite imagery* would be invaluable. Geospatial data is often included in data warehouses for environmental and defense applications.\n",
        "\n",
        "4.  Public sentiment and eyewitness accounts could be scraped from *social media platforms* like Twitter or Facebook. Many businesses use social media data for sentiment analysis and market research.\n",
        "\n",
        "5.  *Traffic Cameras and CCTV* would offer ground-level visual data on zombie movements within cities. In smart cities, traffic and surveillance data are often integrated into a central data warehouse for monitoring and planning.\n",
        "\n",
        "6.  *Weather conditions* could affect the spread of the outbreak and the effectiveness of interventions. Meteorological data is commonly used in various industries like agriculture and retail for predictive analysis.\n",
        "\n",
        "7.  Real-time *news updates* would provide another layer of information, especially from regions where other types of data might be scarce. Media data is often used for trend analysis and is integrated into various types of data warehouses.\n",
        "\n",
        "8.   Data from other *international or governmental organizations* like the WHO could provide a global perspective on the outbreak. In global health research, data from international organizations is often crucial.\n",
        "\n",
        "9.  Information from *utility companies* on power outages or disruptions in water supply could indicate areas with high zombie activity. Utility data is commonly used in municipal planning and emergency response.\n",
        "\n",
        "10. *Air Traffic Control* data about the movement of planes could help in understanding if and how the outbreak is spreading internationally. Transportation data, including air traffic, is often integrated into logistics and planning data warehouses.\n",
        "\n",
        "Each of these data sources would contribute to a multi-dimensional view of the crisis, allowing the CDC to make informed decisions on how best to combat the outbreak."
      ],
      "metadata": {
        "id": "os0olq-Em_9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract, Transform, Load (ETL)\n",
        "\n",
        "Imagine you're at the helm of the CDC's data operations during a Zombie Outbreak. You have a plethora of data coming in from hospital records, military intelligence, satellite imagery, and more. Each of these data sources has its own format, structure, and level of granularity. Some data might be in real-time, while others might be batch-updated every 24 hours. The challenge now is to harmonize this cacophony of data into a symphony of insights. This is the essence of the ETL process.\n",
        "\n",
        "**Extract--** The first step is to extract the data from these various sources. Whether it's pulling in a CSV file from a military server, querying a hospital's database, or using an API to get real-time weather conditions, the extraction process is about getting access to the data.\n",
        "\n",
        "**Transform--** Once you have the raw data, it's rarely in the perfect state for analysis. Maybe the hospital records use one format for dates, while the military intelligence uses another. Perhaps the sentiment data from social media needs to be categorized into positive, neutral, or negative. The transform step is where you clean, enrich, and otherwise prepare the data for its final destination.\n",
        "\n",
        "**Load--** The last step is to load this now-transformed data into your data warehouse. This is where it will live, ready for querying and analysis as you make life-and-death decisions in combating the zombie apocalypse.\n",
        "\n",
        "The ETL process is the logistical backbone that enables organizations to draw actionable insights from a sea of disparate data. Without ETL, your data warehouse is like a library without a catalog: full of information, but nearly impossible to use effectively."
      ],
      "metadata": {
        "id": "iksCkM44p2Tk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand how this works, it will be helpful to consider a few examples:\n",
        "\n",
        "### Example 1: Hospital Records\n",
        "\n",
        "*Extract--*The first step would involve pulling in data from various hospitals about the number of infected individuals, their symptoms, and the effectiveness of treatments. This could be done through an API that the hospitals expose or by accessing a shared database.\n",
        "\n",
        "*Transform--*The raw data might have inconsistencies like different units of measurement (e.g., temperature in Fahrenheit or Celsius) or varying formats for patient IDs. The transformation step would standardize these units and formats. It might also categorize symptoms into broader categories like 'mild,' 'moderate,' or 'severe' to make analysis easier.\n",
        "\n",
        "*Load--*Finally, this cleaned and transformed data would be loaded into the CDC's data warehouse into a table specifically designed for medical records, ready for querying and analysis.\n",
        "\n",
        "### Example 2: Military Intelligence\n",
        "\n",
        "*Extract--*Reports from military units engaged with zombie containment would be sent in various formats: text files, PDFs, or even voice recordings. These would first be extracted and converted into a readable format.\n",
        "\n",
        "*Transform--*The transformation step would involve parsing these different formats into structured data. For example, key information like the number of zombies encountered, the number of soldiers involved, and the geographic coordinates of the engagement would be extracted and standardized.\n",
        "\n",
        "*Load--*This structured data would then be loaded into a separate table in the data warehouse, designed to store military intelligence. This table might have foreign key relationships with other tables, like geographic locations or time data.\n",
        "\n",
        "### Example 3: Social Media Sentiment\n",
        "\n",
        "*Extract--*Public sentiment and eyewitness accounts could be scraped from social media platforms like Twitter or Facebook using their respective APIs.\n",
        "\n",
        "*Transform--*The raw social media data would likely be unstructured text. Natural Language Processing (NLP) algorithms could be applied to categorize the sentiment of each post as 'positive,' 'negative,' or 'neutral.' Geolocation data could also be extracted to determine the location of each post.\n",
        "\n",
        "*Load--*This transformed data would be loaded into a table designed to store public sentiment. This could then be joined with other tables in the data warehouse to provide a multi-dimensional view of the public's reaction to the outbreak and the effectiveness of containment measures.\n",
        "\n",
        "Each of these examples demonstrates how the ETL process takes raw, disparate data and turns it into a structured, queryable form that can provide invaluable insights, especially in a crisis situation like a zombie outbreak."
      ],
      "metadata": {
        "id": "xKJ_ok0ZqUhZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Table: Common Issues in ETL\n",
        "| Common Issues  | Standard Solutions |\n",
        "| --- | --- |\n",
        "| **Missing Values-**-For instance, some hospital records might lack temperature readings for patients. | Imputation methods can be used to fill in missing values, such as using the mean, median, or a specific constant. Alternatively, records with missing values can be excluded from certain analyses. |\n",
        "| **nconsistent Formats--**Military reports might use different date formats like MM-DD-YYYY or YYYY-MM-DD. | Standardize the format during the transformation step, converting all dates to a single, consistent format. |\n",
        "| **Duplicate Records--**Multiple social media posts might be scraped that are essentially the same, perhaps due to retweets or reposts. | Deduplication methods can be applied to remove or consolidate duplicate records. |\n",
        "| **Data Type Mismatch--**A column expected to contain integers has some entries stored as text (e.g., 'ten' instead of 10). | Convert all entries to a consistent data type, possibly flagging or removing entries that cannot be converted. |\n",
        "| **Outliers--**In a dataset of zombie sightings, one entry reports an implausibly large number of zombies. | Outliers can be truncated, transformed, or removed based on the specific requirements of the analysis. |\n",
        "| **Unstructured Data--**Raw text from social media or PDF reports from military intelligence. | Text parsing and Natural Language Processing (NLP) can be used to extract structured information from unstructured data. |\n",
        "| **Data Skew--**Most of the data is from one geographic location, making it unrepresentative. | Data balancing techniques can be applied, or the skew can be accounted for in the analysis phase. |\n",
        "| **Temporal Issues--**Data from different time zones, leading to inconsistencies. | Convert all time data to a standard time zone, usually Coordinated Universal Time (UTC). |"
      ],
      "metadata": {
        "id": "jxWt7LaHrRp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How to Structure Data Warehouses\n",
        "As mentioned above, application databases primarily serve to facilitate transactional operations, which are essential for the day-to-day functioning of various applications. For instance, when a user signs up on a website, makes a purchase, or updates their profile, these actions are handled by an application database. The database is optimized for quick, real-time read and write operations. It's structured to efficiently handle a large number of simple queries that insert, update, or delete individual records. The focus is on data integrity and consistency, ensuring that all transactions are processed reliably. Here, **data normalization**  and **transaction (ACID)** support is highly important. In the context of a Zombie outbreak, for example, we might have the following application databases:\n",
        "\n",
        "1. *Emergency Response System Database--*This database would manage real-time information related to emergency responses, such as the dispatch of medical teams, allocation of medical supplies, and tracking of emergency vehicles.\n",
        "    - Normalization would be crucial to avoid redundancy and ensure data integrity. For example, if an emergency vehicle is dispatched to multiple locations, normalization would prevent the need to enter the vehicle's details multiple times, reducing the risk of errors.\n",
        "    - Transactions are vital to ensure that when an emergency team is dispatched, all relevant tables are updated atomically. For instance, if a medical team is sent to a location, the database should simultaneously update the team's status to 'dispatched' and decrement the available medical teams count. Failure in any part of this transaction could lead to incorrect data, such as dispatching the same team twice.\n",
        "\n",
        "2. *Quarantine Facility Management Database--*This database would manage the occupancy, resource allocation, and status of various quarantine facilities.\n",
        "    - Normalization would help in efficiently managing resources like beds, medical supplies, and personnel. By keeping these details in separate but linked tables, the database can ensure that any changes to resource availability are consistently reflected across all relevant records.\n",
        "\n",
        "    - When admitting a new patient to a quarantine facility, multiple tables like 'Available Beds,' 'Patient Records,' and 'Medical Supplies' would need to be updated. Transactions ensure that these updates are atomic, maintaining data consistency. For example, admitting a patient should only be successful if a bed is available and medical supplies are sufficient.\n",
        "\n",
        "In each of these examples, normalization ensures data integrity and minimizes redundancy, while transactions ensure that all database operations are atomic and consistent, which is crucial for the real-time, high-stakes environment of a Zombie Outbreak.\n"
      ],
      "metadata": {
        "id": "9OVVsPkCshAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why Warehouses are Different\n",
        "\n",
        "Data warehouses, on the other hand, are built for a different purpose: they are optimized for analytical processing. Unlike application databases, which are concerned with individual transactions, data warehouses are designed to handle complex queries that may involve aggregating or comparing large sets of data. These queries are often read-heavy and require scanning multiple tables to generate reports, identify trends, or make forecasts. The primary goal is to enable fast, efficient data retrieval for analysis, often at the expense of quick data modification capabilities.\n",
        "\n",
        "The differing purposes of application databases and data warehouses have significant implications for how they are structured. In an application database, the schema is usually normalized to minimize data redundancy. This makes it easier to maintain data integrity but can make complex queries slower because they may require data to be gathered from multiple tables.\n",
        "\n",
        "In contrast, a data warehouse often employs **denormalized schemas** , like the Star Schema or Snowflake Schema, to speed up query performance. Denormalization involves storing redundant data to reduce the need for complex joins during queries, thereby making data retrieval faster. In our Zombie Outbreak scenario, for example, a denormalized schema might store each patient's full information alongside each medical record, rather than storing it in a separate table. This would make it quicker to generate comprehensive reports on patient statuses.\n",
        "\n",
        "Additionally, data warehouses often use specialized indexing and partitioning strategies to further optimize query performance. Materialized views, which store the results of complex queries for quick retrieval, are another common feature in data warehouses but are rarely used in application databases.\n",
        "\n",
        "In summary, the structure of your data repository---be it an application database or a data warehouse---will be heavily influenced by its primary function. Understanding these functions and their implications is crucial when designing a system, especially one that needs to handle the complexities and urgencies of a situation like a Zombie Outbreak."
      ],
      "metadata": {
        "id": "iqYnoDMNs_B6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Structuring the Data Warehouse: The Role of Denormalization\n",
        "\n",
        "In contrast to application databases, where normalization is often a priority, data warehouses commonly employ Denormalization as a strategy to improve query performance. Denormalization involves combining tables in a way that makes data retrieval more straightforward and faster, even if it means introducing some level of data redundancy. For example, we might choose to (intentionally) keep our tables in 1NF, rather than 3NF.\n",
        "\n",
        "#### What is Denormalization?\n",
        "\n",
        "Denormalization is the process of consolidating multiple tables into fewer tables, often by including redundant data. For example, instead of having a separate table for 'Patient Information' and another for 'Medical Records,' a denormalized table might include columns from both original tables. This reduces the need for complex joins when querying, thereby speeding up data retrieval.\n",
        "\n",
        "#### Why Denormalization?\n",
        "\n",
        "1.  By reducing the need for joins, denormalization makes queries run faster. This is crucial in a high-stakes, time-sensitive scenario like a Zombie Outbreak, where quick data analysis can be a matter of life and death.\n",
        "\n",
        "2.  Denormalization often results in tables that are easier to query because all the data needed for a particular analysis is stored together. This is particularly useful for less technical users who may need to query the data but are not familiar with complex SQL joins.\n",
        "\n",
        "3.  Data warehouses are primarily used for read-heavy operations like data analysis and reporting. Denormalization is aligned with this purpose, as it optimizes for fast read operations at the expense of making write operations more complex due to the redundancy.\n",
        "\n",
        "#### Considerations\n",
        "\n",
        "While denormalization has its advantages, it's not without trade-offs:\n",
        "\n",
        "1.  Storing redundant data increases the risk of inconsistencies. For example, if a patient's address is stored in multiple places, an update to one may not automatically reflect in the others.\n",
        "\n",
        "2.  Redundant data means you'll need more storage space, which could increase costs.\n",
        "\n",
        "3.  Because the same piece of data might exist in multiple places, write and update operations can become more complex and time-consuming.\n",
        "\n",
        "In the context of a Zombie Outbreak, the data warehouse might include denormalized tables that combine patient information with medical records, or military unit details with their activity logs. This would enable quicker generation of comprehensive reports, whether it's to analyze the effectiveness of a treatment or to strategize the deployment of military units."
      ],
      "metadata": {
        "id": "N3zznEkJtiaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using the Star Schema for Data Warehouses\n",
        "\n",
        "After discussing denormalization, which is about keeping related data together to make it easier to manage, we're now going to explore a specific way to organize this data: the Star Schema.\n",
        "\n",
        "#### Why Star Schema?\n",
        "\n",
        "Imagine you're part of a team that's trying to understand a zombie outbreak. You need to know where the zombies are appearing, when they're most active, and how effective your team's response has been. You could keep all this information in one massive table, but that would be overwhelming and inefficient.\n",
        "\n",
        "The Star Schema helps by organizing this data into a central table and several related tables. This makes it easier to find the specific information you need, much like how a well-organized filing cabinet lets you quickly locate important documents.\n",
        "\n",
        "#### How Does It Work?\n",
        "\n",
        "In the Star Schema, the **fact table** is the central table that contains the primary information you're interested in. In our zombie example, this would be the `ZombieEncounters` table. This table holds key metrics like the number of zombies encountered (`NumberOfZombies`) and the number of civilians involved (`CiviliansInvolved`).\n",
        "\n",
        "Then, we have **dimension tables** that provide additional context for each of these encounters. For instance, the `Time` table gives us detailed information about when each encounter occurred, including the date, hour, minute, and second. The `Location` table tells us where the encounter happened, specifying the city, state, and even geographical coordinates like latitude and longitude. The `ResponseTeam` table provides information about the team that responded to the encounter, such as the team's name, size, and any special equipment they used."
      ],
      "metadata": {
        "id": "5guGOkWgvW85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### STAR in SQL\n",
        "In the following SQL code, we're creating these tables using the `CREATE TABLE` command. When we specify `PRIMARY KEY`, we're creating a unique identifier for each record in that table. This is like having a unique code for each document in a filing cabinet, making it easier to find later. The `REFERENCES` keyword establishes a link between the central fact table and the dimension tables. This tells the database how these tables are connected, so we can easily pull information from multiple tables when we need to.\n",
        "\n",
        "By organizing our data this way, the Star Schema makes it easier to perform complex queries and analyses. It allows us to quickly find answers to important questions, like identifying the times and locations with the most zombie activity, or evaluating the effectiveness of different response teams."
      ],
      "metadata": {
        "id": "IiYHwq3svZAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Postgres\n",
        "!apt install postgresql postgresql-contrib &>log\n",
        "!service postgresql start\n",
        "!sudo -u postgres psql -c \"CREATE USER root WITH SUPERUSER\"\n",
        "# set connection\n",
        "%load_ext sql\n",
        "%sql postgresql+psycopg2://@/postgres"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ve94kMbNw8eO",
        "outputId": "49d59747-276a-451d-e32a-8ae1fa93e43b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "CREATE ROLE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "-- Create the Time dimension table\n",
        "DROP TABLE IF EXISTS Time CASCADE;\n",
        "CREATE TABLE Time (\n",
        "    TimeID SERIAL PRIMARY KEY,\n",
        "    Date DATE,\n",
        "    Hour INT,\n",
        "    Minute INT,\n",
        "    Second INT\n",
        ");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZTKBHtvm0r2",
        "outputId": "150866fa-2802-4b02-a033-db4e9548c01f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * postgresql+psycopg2://@/postgres\n",
            "Done.\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "DROP TABLE IF EXISTS Location CASCADE;\n",
        "-- Create the Location dimension table\n",
        "CREATE TABLE Location (\n",
        "    LocationID SERIAL PRIMARY KEY,\n",
        "    City VARCHAR(50),\n",
        "    State VARCHAR(50),\n",
        "    Country VARCHAR(50),\n",
        "    Latitude FLOAT,\n",
        "    Longitude FLOAT\n",
        ");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqgIy61Ixu2H",
        "outputId": "354d0f64-c38b-4fed-8198-275e1e2d29d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * postgresql+psycopg2://@/postgres\n",
            "Done.\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "DROP TABLE IF EXISTS ResponseTeam CASCADE;\n",
        "-- Create the ResponseTeam dimension table\n",
        "CREATE TABLE ResponseTeam (\n",
        "    TeamID SERIAL PRIMARY KEY,\n",
        "    TeamName VARCHAR(50),\n",
        "    TeamSize INT,\n",
        "    SpecialEquipment VARCHAR(100)\n",
        ");"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy5YzxMCx1gd",
        "outputId": "d4ea05cb-5098-4572-e382-c57282eba186"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * postgresql+psycopg2://@/postgres\n",
            "Done.\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "DROP TABLE IF EXISTS ZombieEncounters CASCADE;\n",
        "-- Create the central fact table: ZombieEncounters\n",
        "CREATE TABLE ZombieEncounters (\n",
        "    EncounterID SERIAL PRIMARY KEY,\n",
        "    TimeID INT REFERENCES Time(TimeID),\n",
        "    LocationID INT REFERENCES Location(LocationID),\n",
        "    TeamID INT REFERENCES ResponseTeam(TeamID),\n",
        "    NumberOfZombies INT,\n",
        "    CiviliansInvolved INT,\n",
        "    SuppliesUsed VARCHAR(100)\n",
        ");\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJRFsF3lx8C0",
        "outputId": "5c9a6000-7137-4f3e-a602-9851bb612ebf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * postgresql+psycopg2://@/postgres\n",
            "Done.\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}